{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the file illustrating simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modeules to use\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from sklearn.svm import SVC,SVR\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.ensemble import StackingRegressor, StackingClassifier\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "from scipy import linalg, special\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import statistics as stat\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.ensemble import VotingClassifier, VotingRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocess\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import logistic\n",
    "import os \n",
    "from scipy.linalg import toeplitz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate data\n",
    "def simulation_base(p,n,rho,beta_gn,sz,sz0):\n",
    "    # complex relationship with fixed true model\n",
    "    from itertools import combinations\n",
    "    p0 = 20; prest = p-p0\n",
    "    interind = np.array([np.array(list(x)) for x in combinations(range(2, len(beta_gn), len(beta_gn)//5), 2)])\n",
    "\n",
    "    z00 = np.random.binomial(n=1, p=0.35, size=n)\n",
    "\n",
    "    # correlated continuous covariates\n",
    "    sig = toeplitz([rho ** i for i in range(p0-1)])\n",
    "    z0 = np.random.multivariate_normal(mean=np.zeros(p0-1), cov=sig, size=n)\n",
    "    Z0 = np.concatenate([z00[:, None], z0], axis=1)\n",
    "    interterm = np.apply_along_axis(lambda x: Z0[:, x[0]] * Z0[:, x[1]], axis=1, arr=interind)\n",
    "    costerm = np.apply_along_axis(lambda x: np.cos(Z0[:,x]),axis = 0, arr = np.unique(interind))\n",
    "    puse0 = 1/(1+(np.exp(-0.2*np.dot(Z0, beta_gn)- 0.1 * interterm.sum(axis=0))))\n",
    "    t0 = np.random.binomial(n=1, p=puse0, size=n)\n",
    "\n",
    "    y0_1 = np.random.normal(loc=sz0 + np.dot(Z0, beta_gn)+interterm.sum(axis=0), scale=1)\n",
    "    y0_0 = np.random.normal(loc=np.dot(Z0, beta_gn)+interterm.sum(axis=0)+costerm.sum(axis = 1), scale=1)\n",
    "    y0 = np.where(t0==1, y0_1, y0_0)\n",
    "\n",
    "    Zaux0 = np.random.normal(0,1,size = (n,prest))\n",
    "\n",
    "    dat0 = pd.DataFrame(np.concatenate([Z0, Zaux0, t0[:, None], y0[:, None]], axis=1), columns=[\"z00\"] + [f\"z{i}0\" for i in range(1, p)] + [\"t0\", \"y0\"])\n",
    "\n",
    "    # time point 2\n",
    "    z01 = z00 + np.random.uniform(size=n)\n",
    "\n",
    "    z1_1 = z0 + 0.1*y0_1[:, np.newaxis]\n",
    "    z1_0 = z0 + 0.1*y0_0[:, np.newaxis]\n",
    "    z1 = np.concatenate([z1_1[t0==1, :], z1_0[t0==0, :]], axis=0)\n",
    "\n",
    "    Z1_1 = np.concatenate([z01[:, None], z1_1], axis=1)\n",
    "    Z1_0 = np.concatenate([z01[:, None], z1_0], axis=1)\n",
    "    Z1 = np.concatenate([z01[:, None], z1], axis=1)\n",
    "\n",
    "    interterm1 = np.apply_along_axis(lambda x: Z1[:, x[0]] * Z1[:, x[1]], axis=1, arr=interind)\n",
    "    costerm1 = np.apply_along_axis(lambda x:np.cos(Z1[:,x]),axis = 0, arr = np.unique(interind))\n",
    "    puse1 = 1/(1+np.exp(-0.1*np.dot(Z1, beta_gn)+0.5-0.05*interterm1[range(5,interterm1.shape[0]),:].sum(axis = 0)))\n",
    "    t1 = np.random.binomial(n=1, p=puse1, size=n)\n",
    "\n",
    "    y1_11 = np.random.normal(loc=sz + np.dot(Z1_1, np.concatenate([np.array([0]), beta_gn[1:]]))+0.5*interterm1.sum(axis = 0)+0.5*costerm1.sum(axis = 1), scale=1)\n",
    "    y1_00 = np.random.normal(loc=np.dot(Z1_0, np.concatenate([np.array([0]), beta_gn[1:]]))+0.5*interterm1.sum(axis = 0), scale=1)\n",
    "    y1_10 = np.random.normal(loc=np.dot(Z1_1, np.concatenate([np.array([0]), beta_gn[1:]]))+0.5*interterm1.sum(axis = 0)+0.5*costerm1.sum(axis = 1), scale=1)\n",
    "    y1_01 = np.random.normal(loc=sz + np.dot(Z1_0, np.concatenate([np.array([0]), beta_gn[1:]]))+0.5*interterm1.sum(axis = 0)+0.5*costerm1.sum(axis = 1), scale=1)\n",
    "    y1 = np.where(t0==1, np.where(t1==1, y1_11, y1_10), np.where(t1==0, y1_00, y1_01))\n",
    "\n",
    "    Zaux1 = np.random.normal(0,1,size = (n,prest))\n",
    "\n",
    "    dat1 = pd.DataFrame(np.concatenate([Z1, Zaux1, t1[:, None], y1[:, None]], axis=1), columns=[\"z01\"] + [f\"z{i}1\" for i in range(1, p)] + [\"t1\", \"y1\"])\n",
    "    \n",
    "    full_data = pd.concat([dat0, dat1], axis=1)\n",
    "    return full_data, pd.DataFrame({'y_11': y1_11, 'y_00': y1_00, 'p0': puse0, 'p1':puse1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PS function\n",
    "def ps_train_ensemble(data,Train,Test,features,outcomes,\n",
    "                      include_lg=True, include_el=True, include_xgb=True, include_mlp=False,\n",
    "                      include_l1=False, include_svm=False, include_nb=False,include_rf=False):\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    # Train,Test = train_test_split(data,test_size=0.4,random_state=42)\n",
    "    np.random.seed(233)\n",
    "    models = {}\n",
    "    # base learner 1\n",
    "    if include_lg:\n",
    "        lg = linear_model.LogisticRegression()\n",
    "        lg.fit(data[features],data[outcomes])\n",
    "        models['logit'] = lg\n",
    "\n",
    "    # base learner 2\n",
    "    if include_el:\n",
    "        el_class = linear_model.SGDClassifier(penalty='elasticnet',loss='log')\n",
    "        param_grid_el = {'alpha': [0.0001, 0.001, 0.01, 0.1]}\n",
    "        gs_el = GridSearchCV(estimator=el_class,param_grid=param_grid_el,cv=5,scoring='neg_log_loss')\n",
    "        gs_el.fit(Train[features],Train[outcomes])\n",
    "        el_class.set_params(**gs_el.best_params_)\n",
    "        el_class.fit(Train[features],Train[outcomes])\n",
    "        models['enet'] = el_class\n",
    "\n",
    "    # base learner 3\n",
    "    if include_xgb:\n",
    "        param_grid_G = {\n",
    "        'learning_rate': [0.01, 0.1, 1],\n",
    "         'max_depth': [55,65,80],\n",
    "        # 'n_estimators': [100,200,300],\n",
    "        # 'alpha':[0.05,0.1,0.18],\n",
    "        # 'gamma': [1,1.5,1.8]\n",
    "        }\n",
    "        xgb_cls = xgb.XGBClassifier(objective='binary:logistic')\n",
    "        gs_xgb = GridSearchCV(\n",
    "            estimator=xgb_cls,\n",
    "            param_grid=param_grid_G,\n",
    "            cv=5,\n",
    "            scoring='neg_log_loss'\n",
    "        )\n",
    "        gs_xgb.fit(Train[features], Train[outcomes])\n",
    "        xgb_cls.set_params(**gs_xgb.best_params_)\n",
    "        xgb_cls.fit(Train[features], Train[outcomes])\n",
    "        models['xgb'] = xgb_cls\n",
    "\n",
    "    # base learner 4\n",
    "    if include_mlp:\n",
    "        mlp_cls = MLPClassifier()\n",
    "        param_grid_mlpc = {\n",
    "            'hidden_layer_sizes': [(50,), (100,), (50,50), (100,100)],\n",
    "        }\n",
    "        gs_mlp = GridSearchCV(estimator=mlp_cls,param_grid=param_grid_mlpc,cv=5,scoring=\"neg_log_loss\")\n",
    "        gs_mlp.fit(Train[features],Train[outcomes])\n",
    "        mlp_cls.set_params(**gs_mlp.best_params_)\n",
    "        mlp_cls.fit(Train[features],Train[outcomes])\n",
    "        models['mlp'] = mlp_cls\n",
    "\n",
    "    if include_l1:\n",
    "        l1_cls = linear_model.LogisticRegression(penalty='l1',solver='saga')\n",
    "        param_grid_l1 = {\n",
    "        'C':[0.1,1,5],\n",
    "        #'max_features': [None, 'sqrt', 'log2']\n",
    "        }\n",
    "        gs_l1 = GridSearchCV(estimator=l1_cls,param_grid=param_grid_l1,cv=5,scoring=\"neg_log_loss\")\n",
    "        gs_l1.fit(Train[features],Train[outcomes])\n",
    "        l1_cls.set_params(**gs_l1.best_params_)\n",
    "        l1_cls.fit(Train[features],Train[outcomes])\n",
    "        models['gbr'] = l1_cls\n",
    "\n",
    "    if include_svm:\n",
    "        svm_c = SVC(kernel='linear',probability=True)\n",
    "        gs_svm = GridSearchCV(estimator=svm_c,param_grid={'C': [0.1,1,10,100]},cv=5,scoring='neg_log_loss')\n",
    "        gs_svm.fit(Train[features],Train[outcomes])\n",
    "        svm_c.set_params(**gs_svm.best_params_)\n",
    "        svm_c.fit(Train[features],Train[outcomes])\n",
    "        models['svm'] = svm_c\n",
    "        \n",
    "    if include_nb:\n",
    "        nb_c = GaussianNB()\n",
    "        param_grid_nb = {\n",
    "            'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "            }\n",
    "        gs_nb = GridSearchCV(nb_c, param_grid=param_grid_nb, cv=5)\n",
    "        gs_nb.fit(Train[features],Train[outcomes])\n",
    "        nb_c.set_params(**gs_nb.best_params_)\n",
    "        nb_c.fit(Train[features],Train[outcomes])\n",
    "        models['nb'] = nb_c\n",
    "\n",
    "    if include_rf:\n",
    "        rf_c = RandomForestClassifier(random_state = 42)\n",
    "        param_grid_rf = {\n",
    "            'n_estimators':[100,200],\n",
    "            'max_depth':[40,50],\n",
    "            'min_samples_split':[2,5],\n",
    "            }\n",
    "        gs_rf = GridSearchCV(rf_c, param_grid=param_grid_rf, cv=5, scoring='neg_log_loss')\n",
    "        gs_rf.fit(Train[features],Train[outcomes])\n",
    "        rf_c.set_params(**gs_rf.best_params_)\n",
    "        rf_c.fit(Train[features],Train[outcomes])\n",
    "        models['rf'] = rf_c\n",
    "\n",
    "    # meta learner\n",
    "    Q1fit = []\n",
    "    if include_lg:\n",
    "        Q1fit.append(lg.predict_proba(Test[features])[:,1].ravel())\n",
    "    if include_el:\n",
    "        Q1fit.append(el_class.predict_proba(Test[features])[:,1].ravel())\n",
    "    if include_xgb:\n",
    "        Q1fit.append(xgb_cls.predict_proba(Test[features])[:,1].ravel())\n",
    "    if include_mlp:\n",
    "        Q1fit.append(mlp_cls.predict_proba(Test[features])[:,1].ravel())\n",
    "    if include_l1:\n",
    "        Q1fit.append(l1_cls.predict_proba(Test[features])[:,1].ravel())\n",
    "    if include_svm:\n",
    "        Q1fit.append(svm_c.predict_proba(Test[features])[:,1].ravel())\n",
    "    if include_nb:\n",
    "        Q1fit.append(nb_c.predict_proba(Test[features])[:,1].ravel())\n",
    "    if include_rf:\n",
    "        Q1fit.append(rf_c.predict_proba(Test[features])[:,1].ravel())\n",
    "\n",
    "    cls_stack = linear_model.LogisticRegression()\n",
    "    cls_stack.fit(np.transpose(Q1fit),Test[outcomes])\n",
    "\n",
    "    return models, cls_stack\n",
    "\n",
    "def ps_function(data,features,outcomes, lg=True, el=True, xgb=True, mlp=False,\n",
    "                l1=False, svm=False, nb=False,rf=False):\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    Train,Test = train_test_split(data,test_size=0.4,random_state=42)\n",
    "    model_tr,stack_tr = ps_train_ensemble(data,Train,Test,features,outcomes, include_lg=lg, include_el=el, include_xgb=xgb, include_mlp=mlp,\n",
    "                                          include_l1=l1, include_svm=svm, include_nb=nb,include_rf=rf)\n",
    "    model_ts,stack_ts = ps_train_ensemble(data,Test,Train,features,outcomes, include_lg=lg, include_el=el, include_xgb=xgb, include_mlp=mlp,\n",
    "                                          include_l1=l1, include_svm=svm, include_nb=nb,include_rf=rf)\n",
    "    return model_tr,model_ts,stack_tr,stack_ts\n",
    "\n",
    "def ps_predict(preddat,features,model_tr,model_ts,stack_tr,stack_ts):\n",
    "    y_pred_tr = {}\n",
    "    for name, model in model_tr.items():\n",
    "        y_pred_tr[name] = model.predict_proba(preddat[features])[:,1].ravel()\n",
    "    Q_base_tr = np.transpose(np.array(list(y_pred_tr.values())))\n",
    "    eta_tr = stack_tr.predict_proba(Q_base_tr)[:,1]\n",
    "\n",
    "    y_pred_ts = {}\n",
    "    for name, model in model_ts.items():\n",
    "        y_pred_ts[name] = model.predict_proba(preddat[features])[:,1].ravel()\n",
    "    Q_base_ts = np.transpose(np.array(list(y_pred_ts.values())))\n",
    "    eta_ts = stack_ts.predict_proba(Q_base_ts)[:,1]\n",
    "\n",
    "    return eta_tr,eta_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gcomputation function\n",
    "def gcompu_train_ensemble(data,Train,Test,features,outcomes,\n",
    "                          include_lm = True, include_rg = False, \n",
    "                          include_xgb = True, include_mlp = False,\n",
    "                          include_gbr = False):\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    # Train,Test = train_test_split(data,test_size=0.4,random_state=42)\n",
    "    np.random.seed(233)\n",
    "    models = {}\n",
    "    # base learner 1\n",
    "    if include_lm:\n",
    "        lm_1 = linear_model.LinearRegression()\n",
    "        lm_1.fit(data[features],data[outcomes])\n",
    "        models['lm'] = lm_1\n",
    "\n",
    "    # base learner 2\n",
    "    if include_rg:\n",
    "        rg_lm = linear_model.Ridge()\n",
    "        param_grid_rg = {'alpha': [0.1, 1, 10]}\n",
    "        gs_rg = GridSearchCV(estimator=rg_lm,param_grid=param_grid_rg,cv=5,scoring=\"neg_mean_absolute_error\")\n",
    "        gs_rg.fit(Train[features],Train[outcomes])\n",
    "        rg_lm.set_params(**gs_rg.best_params_)\n",
    "        rg_lm.fit(Train[features],Train[outcomes])\n",
    "        models['rg'] = rg_lm\n",
    "\n",
    "    # base learner 3\n",
    "    if include_xgb:\n",
    "        param_grid_G = {\n",
    "        'learning_rate': [0.01, 0.1, 1],\n",
    "        'max_depth': [15,20,25,35],\n",
    "        # 'n_estimators': [100, 150,200],\n",
    "        #'alpha':[0.05,0.1,0.18],\n",
    "        #'gamma': [1,1.5,1.8]\n",
    "        }\n",
    "        xgb_lm1_tr = xgb.XGBRegressor()\n",
    "        gs_gcomp1_tr = GridSearchCV(\n",
    "            estimator=xgb_lm1_tr,\n",
    "            param_grid=param_grid_G,\n",
    "            cv=5,\n",
    "            scoring='neg_mean_absolute_error'\n",
    "        )\n",
    "        gs_gcomp1_tr.fit(Train[features], Train[outcomes])\n",
    "        xgb_lm1_tr.set_params(**gs_gcomp1_tr.best_params_)\n",
    "        xgb_lm1_tr.fit(Train[features], Train[outcomes])\n",
    "        models['xgb'] = xgb_lm1_tr\n",
    "\n",
    "    # base learner 4\n",
    "    if include_mlp:\n",
    "        mlp_lm = MLPRegressor()\n",
    "        param_grid_mlpr = {\n",
    "            'hidden_layer_sizes': [ (100,), (150,), (100, 100), (150,150)],\n",
    "            'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "        }\n",
    "        gs_mlp_lm = GridSearchCV(estimator=mlp_lm,param_grid=param_grid_mlpr,cv=5,scoring=\"neg_mean_absolute_error\")\n",
    "        gs_mlp_lm.fit(Train[features],Train[outcomes])\n",
    "        mlp_lm.set_params(**gs_mlp_lm.best_params_)\n",
    "        mlp_lm.fit(Train[features],Train[outcomes])\n",
    "        models['mlp'] = mlp_lm\n",
    "\n",
    "    if include_gbr:\n",
    "        gbr_lm = GradientBoostingRegressor()\n",
    "        param_grid_gbr = {\n",
    "        # 'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'max_depth': [15,20,25,35],\n",
    "        #'min_samples_split': [2, 3, 4],\n",
    "        #'min_samples_leaf': [1, 2, 3],\n",
    "        #'max_features': [None, 'sqrt', 'log2']\n",
    "        }\n",
    "        gs_gbr_lm = GridSearchCV(estimator=gbr_lm,param_grid=param_grid_gbr,cv=5,scoring=\"neg_mean_absolute_error\")\n",
    "        gs_gbr_lm.fit(Train[features],Train[outcomes])\n",
    "        gbr_lm.set_params(**gs_gbr_lm.best_params_)\n",
    "        gbr_lm.fit(Train[features],Train[outcomes])\n",
    "        models['gbr'] = gbr_lm\n",
    "\n",
    "    # meta learner\n",
    "    Q1fit = []\n",
    "    if include_lm:\n",
    "        Q1fit.append(lm_1.predict(Test[features]).ravel())\n",
    "    if include_rg:\n",
    "        Q1fit.append(rg_lm.predict(Test[features]).ravel())\n",
    "    if include_xgb:\n",
    "        Q1fit.append(xgb_lm1_tr.predict(Test[features]).ravel())\n",
    "    if include_mlp:\n",
    "        Q1fit.append(mlp_lm.predict(Test[features]).ravel())\n",
    "    if include_gbr:\n",
    "        Q1fit.append(gbr_lm.predict(Test[features]).ravel())\n",
    "\n",
    "    lm_stack = linear_model.LinearRegression()\n",
    "    lm_stack.fit(np.transpose(Q1fit),Test[outcomes])\n",
    "\n",
    "    return models, lm_stack\n",
    "\n",
    "def gcompu_function(data,features,outcomes,lm = True, rg = False, \n",
    "                    xgb = True, mlp = False,gbr = False):\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    Train,Test = train_test_split(data,test_size=0.4,random_state=42)\n",
    "    model_tr,stack_tr = gcompu_train_ensemble(data,Train,Test,features,outcomes,\n",
    "                                              include_lm = lm, include_rg = rg, \n",
    "                                              include_xgb = xgb, include_mlp = mlp,include_gbr = gbr)\n",
    "    model_ts,stack_ts = gcompu_train_ensemble(data,Test,Train,features,outcomes,\n",
    "                                              include_lm = lm, include_rg = rg, \n",
    "                                              include_xgb = xgb, include_mlp = mlp,include_gbr = gbr)\n",
    "    return model_tr,model_ts,stack_tr,stack_ts\n",
    "\n",
    "\n",
    "def gcompu_predict(preddat,features,model_tr,model_ts,stack_tr,stack_ts):\n",
    "    y_pred_tr = {}\n",
    "    for name, model in model_tr.items():\n",
    "        y_pred_tr[name] = model.predict(preddat[features]).ravel()\n",
    "    Q_base_tr = np.transpose(np.array(list(y_pred_tr.values())))\n",
    "    eta_tr = stack_tr.predict(Q_base_tr)\n",
    "    y_pred_ts = {}\n",
    "    for name, model in model_ts.items():\n",
    "        y_pred_ts[name] = model.predict(preddat[features]).ravel()\n",
    "    Q_base_ts = np.transpose(np.array(list(y_pred_ts.values())))\n",
    "    eta_ts = stack_ts.predict(Q_base_ts)\n",
    "\n",
    "    return eta_tr,eta_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'yourfilename'\n",
    "np.random.seed(123)\n",
    "p0 = 20; \n",
    "beta_gn = np.zeros(p0)\n",
    "selected_indices = np.random.choice(np.arange(p0), size=p0-10, replace=False)\n",
    "beta_gn[selected_indices] = np.random.normal(size=p0-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation_loop(b,n = 1000,p = 50,filename = filename,beta_gn = beta_gn):\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    print(p)\n",
    "    np.random.seed(123+b)\n",
    "    testdat, paramdat = simulation_base(p,n,0.5,beta_gn,1.8,0.5)\n",
    "    filename = f'{filename}simu{b}.csv'\n",
    "    testdat.to_csv(filename, index=False)\n",
    "    phi_data = (np.mean(paramdat['y_11']-paramdat['y_00']))\n",
    "\n",
    "    # estimate pt1 ps\n",
    "    ps0outcome = ['t0']\n",
    "    ps0feature = ['z' + str(x) +'0' for x in range(0,p)]\n",
    "    ps1outcome = ['t1']\n",
    "    ps1feature = np.concatenate((['z' + str(x) +'0' for x in range(0,p)],\n",
    "                                    ['z' + str(x) +'1' for x in range(0,p)],['t0','y0']))\n",
    "    #Train,Test = train_test_split(testdat,test_size = 0.4,random_state=42)\n",
    "    \n",
    "\n",
    "    baseps0_tr,baseps0_ts,cls_stack0_tr,cls_stack0_ts = ps_function(testdat,ps0feature,ps0outcome,lg=False,l1=True)\n",
    "    enps0_tr,enps0_ts = ps_predict(testdat,ps0feature,baseps0_tr,baseps0_ts,cls_stack0_tr,cls_stack0_ts)\n",
    "\n",
    "    baseps1_tr,baseps1_ts,cls_stack1_tr,cls_stack1_ts = ps_function(testdat,ps1feature,ps1outcome,lg=False,l1=True)\n",
    "    enps1_tr,enps1_ts = ps_predict(testdat,ps1feature,baseps1_tr,baseps1_ts,cls_stack1_tr,cls_stack1_ts)\n",
    "\n",
    "    # MSM with original ps\n",
    "    t0 = testdat['t0']; t1= testdat['t1']; y1 = testdat['y1']; y0 = testdat['y0']\n",
    "    IPW = (t0/((enps0_tr+enps0_ts)/2)+(1-t0)/(1-((enps0_tr+enps0_ts)/2)))*(t1/((enps1_tr+enps1_ts)/2)+(1-t1)/(1-((enps1_tr+enps1_ts)/2)))\n",
    "    msm_lm = linear_model.LinearRegression()\n",
    "    msm_lm.fit(testdat[['t0','t1']],testdat['y1'],sample_weight=IPW)\n",
    "    phi_msm_enl = (np.sum(msm_lm.coef_))\n",
    "    # SEs\n",
    "    predictions = msm_lm.predict(testdat[['t0', 't1']])\n",
    "    residuals = testdat['y1'] - predictions\n",
    "    # Calculate the variance-covariance matrix\n",
    "    # X is the matrix of predictors with a column of ones for the intercept\n",
    "    Xse = np.column_stack((np.ones(len(testdat)), testdat[['t0', 't1']]))\n",
    "    w_res = residuals**2 * IPW  # Weighted residuals squared\n",
    "    X_weighted = Xse.T * IPW.ravel()  # Apply weights to the predictors matrix\n",
    "    # Variance-covariance matrix calculation\n",
    "    cov_matrix = np.linalg.inv(X_weighted @ Xse) * (X_weighted @ np.diag(w_res) @ Xse).sum()/n\n",
    "    msmenl_se = np.sqrt(cov_matrix[1,1]+cov_matrix[2,2]+2*cov_matrix[1,2])\n",
    "\n",
    "    # estimating weights with logistic regression\n",
    "    lg0 = linear_model.LogisticRegression().fit(testdat[ps0feature],testdat[ps0outcome])\n",
    "    lmps0 = np.clip(lg0.predict_proba(testdat[ps0feature])[:,1].ravel(),0.01,0.99)\n",
    "    lg1 = linear_model.LogisticRegression().fit(testdat[ps1feature],testdat[ps1outcome])\n",
    "    lmps1 = np.clip(lg1.predict_proba(testdat[ps1feature])[:,1].ravel(),0.01,0.99)\n",
    "    lmP = (t0/lmps0+(1-t0)/(1-lmps0))*(t1/lmps1+(1-t1)/(1-lmps1))\n",
    "    msm_lg = linear_model.LinearRegression().fit(testdat[['t0','t1']],testdat['y1'],sample_weight=lmP)\n",
    "    phi_msm_lm = np.sum(msm_lg.coef_)\n",
    "    # SEs\n",
    "    lmpredictions = msm_lg.predict(testdat[['t0', 't1']])\n",
    "    lmresiduals = testdat['y1'] - lmpredictions\n",
    "    # Calculate the variance-covariance matrix\n",
    "    lmw_res = lmresiduals**2 * lmP  # Weighted residuals squared\n",
    "    lmX_weighted = Xse.T * lmP.ravel()  # Apply weights to the predictors matrix\n",
    "    # Variance-covariance matrix calculation\n",
    "    lmcov_matrix = np.linalg.inv(lmX_weighted @ Xse) * (lmX_weighted @ np.diag(lmw_res) @ Xse).sum()/n\n",
    "    msmlm_se = np.sqrt(lmcov_matrix[1,1]+lmcov_matrix[2,2]+2*lmcov_matrix[1,2])\n",
    "\n",
    "    ## outcome regression ensemble\n",
    "    iter1feature = ['z{}{}'.format(i, j) for i in range(p) for j in range(2)] + ['y0', 't0', 't1']\n",
    "    iter1outcome = ['y1']\n",
    "    iter0feature = ['t0']+['z' + str(x) +'0' for x in range(0,p)]\n",
    "\n",
    "    preddat11 = testdat.copy(); preddat11['t0']=1;preddat11['t1']=1\n",
    "    preddat00 = testdat.copy(); preddat00['t0']=0;preddat00['t1']=0\n",
    "    \n",
    "    basemodel1_tr,basemodel1_ts,lm_stack1_tr,lm_stack1_ts = gcompu_function(testdat,iter1feature,iter1outcome,gbr=True)\n",
    "\n",
    "    eta2_tr,eta2_ts = gcompu_predict(testdat,iter1feature,basemodel1_tr,basemodel1_ts,lm_stack1_tr,lm_stack1_ts)\n",
    "    \n",
    "\n",
    "     ## G-comp with LR\n",
    "    lm1 = linear_model.LinearRegression()\n",
    "    lm1.fit(testdat[iter1feature],testdat[iter1outcome])\n",
    "    Q11 = lm1.predict(preddat11[iter1feature])\n",
    "    Q00 = lm1.predict(preddat00[iter1feature])\n",
    "\n",
    "    lm0_1 = linear_model.LinearRegression()\n",
    "    lm0_1.fit(testdat[iter0feature],Q11)\n",
    "    lm0_0 = linear_model.LinearRegression()\n",
    "    lm0_0.fit(testdat[iter0feature],Q00)\n",
    "    phi_gcomp_lm = (np.mean(lm0_1.predict(preddat11[iter0feature])-lm0_0.predict(preddat00[iter0feature])))\n",
    "\n",
    "    ## DR\n",
    "    \n",
    "    #eta2 = lm_stack.predict(Q1fit)\n",
    "\n",
    "    preddat1 = testdat.copy(); preddat1['t1']=1\n",
    "    preddat0 = testdat.copy(); preddat0['t1']=0\n",
    "\n",
    "    eta20_tr,eta20_ts = gcompu_predict(preddat0,iter1feature,basemodel1_tr,basemodel1_ts,lm_stack1_tr,lm_stack1_ts)\n",
    "    \n",
    "    eta20 = (eta20_tr+eta20_ts)/2\n",
    "    basemodel20_tr,basemodel20_ts,lm_stack20_tr,lm_stack20_ts = gcompu_function(pd.concat([testdat, pd.DataFrame(eta20.reshape(n,), columns=['eta20'])], axis=1),\n",
    "                                                    iter0feature,'eta20',gbr=True)\n",
    "\n",
    "    eta21_tr,eta21_ts = gcompu_predict(preddat1,iter1feature,basemodel1_tr,basemodel1_ts,lm_stack1_tr,lm_stack1_ts)\n",
    "    \n",
    "    eta21 = (eta21_tr+eta21_ts)/2\n",
    "    basemodel21_tr,basemodel21_ts,lm_stack21_tr,lm_stack21_ts = gcompu_function(pd.concat([testdat, pd.DataFrame(eta21.reshape(n,), columns=['eta21'])], axis=1),\n",
    "                                                    iter0feature,'eta21',gbr=True)\n",
    "\n",
    "\n",
    "    Q_00_tr,Q_00_ts= gcompu_predict(preddat00,iter0feature,basemodel20_tr,basemodel20_ts,lm_stack20_tr,lm_stack20_ts)\n",
    "\n",
    "    Q_11_tr,Q_11_ts= gcompu_predict(preddat11,iter0feature,basemodel21_tr,basemodel21_ts,lm_stack21_tr,lm_stack21_ts)\n",
    "\n",
    "\n",
    "    # phi_gcomp = (np.mean(Q_11-Q_00))\n",
    "            \n",
    "    eta10_tr,eta10_ts = gcompu_predict(preddat0,iter0feature,basemodel20_tr,basemodel20_ts,lm_stack20_tr,lm_stack20_ts)\n",
    "\n",
    "    eta11_tr,eta11_ts = gcompu_predict(preddat1,iter0feature,basemodel21_tr,basemodel21_ts,lm_stack21_tr,lm_stack21_ts)\n",
    "\n",
    "    eta0_00_tr = Q_00_tr; eta0_00_ts = Q_00_ts\n",
    "\n",
    "    preddat10 = testdat.copy(); preddat10['t0']=1;preddat10['t1']=0\n",
    "    eta0_10_tr,eta0_10_ts = gcompu_predict(preddat10,iter0feature,basemodel20_tr,basemodel20_ts,lm_stack20_tr,lm_stack20_ts)\n",
    "\n",
    "    eta0_11_tr = Q_11_tr; eta0_11_ts = Q_11_ts\n",
    "\n",
    "    preddat01 = testdat.copy(); preddat01['t0']=0;preddat01['t1']=1\n",
    "    eta0_01_tr,eta0_01_ts = gcompu_predict(preddat01,iter0feature,basemodel21_tr,basemodel21_ts,lm_stack21_tr,lm_stack21_ts)\n",
    "\n",
    "\n",
    "    ## linear equation for DR, subject to change based on MSM/other outcome equation of interest\n",
    "    # t0 = testdat['t0']; t1= testdat['t1']; y1 = testdat['y1']; y0 = testdat['y0']\n",
    "    # enps0 = lmps0; enps1 = lmps1\n",
    "    b0 = [4,2,2];b1 = [2,2,1];b2 = [2,1,2]\n",
    "    f01_tr = ((t0/enps0_tr+(1-t0)/(1-enps0_tr))*(t1/enps1_tr+(1-t1)/(1-enps1_tr))*(y1-eta2_tr.ravel()))\n",
    "    f02_tr = 1/((enps0_tr**t0)*(1-enps0_tr)**(1-t0))*(eta20_tr.ravel()-eta10_tr.ravel())+1/((enps0_tr**t0)*(1-enps0_tr)**(1-t0))*(eta21_tr.ravel()-eta11_tr.ravel())\n",
    "    f03_tr = (eta0_00_tr+eta0_11_tr+eta0_10_tr.ravel()+eta0_01_tr.ravel())\n",
    "    c0_tr = np.mean(f01_tr+f02_tr+f03_tr)\n",
    "    \n",
    "\n",
    "    f11_tr = (t0/enps0_tr+(1-t0)/(1-enps0_tr))*(t1/enps1_tr+(1-t1)/(1-enps1_tr))*(y1-eta2_tr.ravel())*t0\n",
    "    f12_tr = t0/((enps0_tr**t0)*(1-enps0_tr)**(1-t0))*(eta20_tr.ravel()-eta10_tr.ravel())+t0/((enps0_tr**t0)*(1-enps0_tr)**(1-t0))*(eta21_tr.ravel()-eta11_tr.ravel())\n",
    "    f13_tr = (eta0_11_tr+eta0_10_tr.ravel())\n",
    "    c1_tr = np.mean(f11_tr+f12_tr+f13_tr)\n",
    "\n",
    "    f21_tr = (t0/enps0_tr+(1-t0)/(1-enps0_tr))*(t1/enps1_tr+(1-t1)/(1-enps1_tr))*(y1-eta2_tr.ravel())*t1\n",
    "    f22_tr = 1/((enps0_tr**t0)*(1-enps0_tr)**(1-t0))*(eta21_tr.ravel()-eta11_tr.ravel())\n",
    "    f23_tr = (eta0_11_tr+eta0_01_tr.ravel())\n",
    "    c2_tr = np.mean(f21_tr+f22_tr+f23_tr)\n",
    "\n",
    "    coef_tr = np.linalg.solve(np.array([b0,b1,b2]),np.array([c0_tr,c1_tr,c2_tr]))\n",
    "\n",
    "    # training set\n",
    "    f01_ts = ((t0/enps0_ts+(1-t0)/(1-enps0_ts))*(t1/enps1_ts+(1-t1)/(1-enps1_ts))*(y1-eta2_ts.ravel()))\n",
    "    f02_ts = 1/((enps0_ts**t0)*(1-enps0_ts)**(1-t0))*(eta20_ts.ravel()-eta10_ts.ravel())+1/((enps0_ts**t0)*(1-enps0_ts)**(1-t0))*(eta21_ts.ravel()-eta11_ts.ravel())\n",
    "    f03_ts = (eta0_00_ts+eta0_11_ts+eta0_10_ts.ravel()+eta0_01_ts.ravel())\n",
    "    c0_ts = np.mean(f01_ts+f02_ts+f03_ts)\n",
    "\n",
    "    f11_ts = (t0/enps0_ts+(1-t0)/(1-enps0_ts))*(t1/enps1_ts+(1-t1)/(1-enps1_ts))*(y1-eta2_ts.ravel())*t0\n",
    "    f12_ts = t0/((enps0_ts**t0)*(1-enps0_ts)**(1-t0))*(eta20_ts.ravel()-eta10_ts.ravel())+t0/((enps0_ts**t0)*(1-enps0_ts)**(1-t0))*(eta21_ts.ravel()-eta11_ts.ravel())\n",
    "    f13_ts = (eta0_11_ts+eta0_10_ts.ravel())\n",
    "    c1_ts = np.mean(f11_ts+f12_ts+f13_ts)\n",
    "\n",
    "    f21_ts = (t0/enps0_ts+(1-t0)/(1-enps0_ts))*(t1/enps1_ts+(1-t1)/(1-enps1_ts))*(y1-eta2_ts.ravel())*t1\n",
    "    f22_ts = 1/((enps0_ts**t0)*(1-enps0_ts)**(1-t0))*(eta21_ts.ravel()-eta11_ts.ravel())\n",
    "    f23_ts = (eta0_11_ts+eta0_01_ts.ravel())\n",
    "    c2_ts = np.mean(f21_ts+f22_ts+f23_ts)\n",
    "\n",
    "    coef_ts = np.linalg.solve(np.array([b0,b1,b2]),np.array([c0_ts,c1_ts,c2_ts]))\n",
    "\n",
    "    phi_dr = (coef_tr[1]+coef_ts[1])/2+(coef_tr[2]+coef_ts[2])/2\n",
    "\n",
    "    # empirical sandwitch variance for DR\n",
    "    Bmat = np.array([[-4,-2,-2],\n",
    "                     [-2,-2,-1],\n",
    "                     [-2,-1,-2]])\n",
    "    Bmatinv = np.linalg.inv(Bmat)\n",
    "    phi1 = (f01_tr+f01_ts)/2+(f02_tr+f02_ts)/2+(f03_tr+f03_ts)/2-4*(coef_tr[0]+coef_ts[0])/2-2*(coef_tr[1]+coef_ts[1])/2-2*(coef_tr[2]+coef_ts[2])/2\n",
    "    phi2 = (f11_tr+f11_ts)/2+(f12_tr+f12_ts)/2+(f13_tr+f13_ts)/2-2*(coef_tr[0]+coef_ts[0])/2-2*(coef_tr[1]+coef_ts[1])/2-(coef_tr[2]+coef_ts[2])/2\n",
    "    phi3 = (f21_tr+f21_ts)/2+(f22_tr+f22_ts)/2+(f23_tr+f23_ts)/2-2*(coef_tr[0]+coef_ts[0])/2-(coef_tr[1]+coef_ts[1])/2-2*(coef_tr[2]+coef_ts[2])/2\n",
    "    Phi = np.array([phi1,phi2,phi3])\n",
    "    Fmat = np.zeros((3,3))\n",
    "    for i in range(n):\n",
    "        phi_vector = Phi[:,i]\n",
    "        outer_product = np.outer(phi_vector,phi_vector)\n",
    "        Fmat += outer_product\n",
    "    Fmat /= n\n",
    "    Vmat = (Bmatinv @ Fmat @ Bmatinv.T)/n\n",
    "    dr_se  = np.sqrt(Vmat[1,1]+Vmat[2,2]+2*Vmat[1,2])\n",
    "\n",
    "    # SE from Chernozhukov paper\n",
    "    phi1_dr = (f01_tr)+(f02_tr)+(f03_tr)-4*(coef_tr[0])-2*(coef_tr[1])-2*(coef_tr[2])\n",
    "    phi2_dr = (f11_tr)+(f12_tr)+(f13_tr)-2*(coef_tr[0])-2*(coef_tr[1])-(coef_tr[2])\n",
    "    phi3_dr = (f21_tr)+(f22_tr)+(f23_tr)-2*(coef_tr[0])-(coef_tr[1])-2*(coef_tr[2])\n",
    "    phi1_ds = (f01_ts)+(f02_ts)+(f03_ts)-4*(coef_ts[0])-2*(coef_ts[1])-2*(coef_ts[2])\n",
    "    phi2_ds = (f11_ts)+(f12_ts)+(f13_ts)-2*(coef_ts[0])-2*(coef_ts[1])-(coef_ts[2])\n",
    "    phi3_ds = (f21_ts)+(f22_ts)+(f23_ts)-2*(coef_ts[0])-(coef_ts[1])-2*(coef_ts[2])\n",
    "    Phi = np.column_stack([np.concatenate([phi1_dr,phi1_ds]),np.concatenate([phi2_dr,phi2_ds]),np.concatenate([phi3_dr,phi3_ds])])\n",
    "    Sigma = np.dot(Phi.T,Phi)/(2*n)\n",
    "    dr_se_chern = np.sqrt((Sigma[1,1]+Sigma[2,2]+2*Sigma[1,2])/(2*n))\n",
    "\n",
    "    \n",
    "\n",
    "    # bootstrap se for Lm Gcomp\n",
    "    gcomplm_boot = [];msmlm_boot = []\n",
    "    for i in range(500):\n",
    "        sampleid = np.random.choice(range(1, n+1), size=n, replace=True)\n",
    "        dat_boot = testdat.iloc[sampleid-1, :]\n",
    "\n",
    "        bootdat11 = dat_boot.copy(); bootdat11['t0']=1;bootdat11['t1']=1\n",
    "        bootdat00 = dat_boot.copy(); bootdat00['t0']=0;bootdat00['t1']=0\n",
    "\n",
    "        ## G-comp with LR\n",
    "        lm1boot = linear_model.LinearRegression()\n",
    "        lm1boot.fit(dat_boot[iter1feature],dat_boot[iter1outcome])\n",
    "        Q11boot = lm1boot.predict(bootdat11[iter1feature])\n",
    "        Q00boot = lm1boot.predict(bootdat00[iter1feature])\n",
    "\n",
    "        lm0_1boot = linear_model.LinearRegression()\n",
    "        lm0_1boot.fit(dat_boot[iter0feature],Q11boot)\n",
    "        lm0_0boot = linear_model.LinearRegression()\n",
    "        lm0_0boot.fit(dat_boot[iter0feature],Q00boot)\n",
    "        gcomplm_boot.append(np.mean(lm0_1boot.predict(bootdat11[iter0feature])-lm0_0boot.predict(bootdat00[iter0feature])))\n",
    "\n",
    "        ## MSM with LR\n",
    "        lgboot0 = linear_model.LogisticRegression().fit(dat_boot[ps0feature],dat_boot[ps0outcome])\n",
    "        lmpsboot0 = np.clip(lgboot0.predict_proba(dat_boot[ps0feature])[:,1].ravel(),0.01,0.99)\n",
    "        lgboot1 = linear_model.LogisticRegression().fit(dat_boot[ps1feature],dat_boot[ps1outcome])\n",
    "        lmpsboot1 = np.clip(lgboot1.predict_proba(dat_boot[ps1feature])[:,1].ravel(),0.01,0.99)\n",
    "        lmPboot = (dat_boot['t0']/lmpsboot0+(1-dat_boot['t0'])/(1-lmpsboot0))*(dat_boot['t1']/lmpsboot1+(1-dat_boot['t1'])/(1-lmpsboot1))\n",
    "        msm_lgboot = linear_model.LinearRegression().fit(dat_boot[['t0','t1']],dat_boot['y1'],sample_weight=lmPboot)\n",
    "        msmlm_boot.append(np.sum(msm_lgboot.coef_))\n",
    "\n",
    "    gcomplm_se = np.std(gcomplm_boot);msmlmb_se = np.std(msmlm_boot)\n",
    "\n",
    "\n",
    "    return  phi_data, phi_dr,phi_gcomp_lm, dr_se,gcomplm_se,phi_msm_enl,msmenl_se,phi_msm_lm,msmlm_se,msmlmb_se,dr_se_chern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from functools import partial\n",
    "start_time = time.time()\n",
    "with multiprocess.Pool(processes=20) as pool:\n",
    "    # phi_data_100bs, phi_gcomp_100bs, phi_dr_100bs, phi_gcomp_lm_100bs, phi_msm_lm_100bs, dr_boot_100bs, dr_btwo_100bs = pool.map(simulation_loop,range(10))\n",
    "    results1_5 = pool.map(partial(simulation_loop,filename = filename,p = 50),range(10))\n",
    "    pool.close(); pool.join()\n",
    "\n",
    "end_time = time.time()\n",
    "end_time-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
